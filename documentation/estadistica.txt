# py eda.py                                       
                                                                                                       
Label mapping: {'Background': 0, 'Building-Flooded': 1, 'Building-Non-Flooded': 2, 'Road-Flooded': 3, 'Road-Non-Flooded': 4, 'Water': 5, 'Tree': 6, 'Vehicle': 7, 'Pool': 8, 'Grass': 9}

Dataset splits:
  Train: 630 samples
  Val:   135 samples
  Test:  135 samples
DataLoaders created successfully!

=== TRAIN ===
Batches: 20
Images: 630
Image sizes (C,H,W): {(3, 640, 640): 630}
Mask sizes (H,W): {torch.Size([640, 640]): 630}
Pixel distribution by class id:
  0: 3448556 pixels (1.34%)
  1: 3998459 pixels (1.55%)
  2: 8835088 pixels (3.42%)
  3: 6582870 pixels (2.55%)
  4: 14112604 pixels (5.47%)
  5: 30572642 pixels (11.85%)
  6: 54300759 pixels (21.04%)
  7: 473789 pixels (0.18%)
  8: 667534 pixels (0.26%)
  9: 135055699 pixels (52.34%)

=== VAL ===
Batches: 5
Images: 135
Image sizes (C,H,W): {(3, 640, 640): 135}
Mask sizes (H,W): {torch.Size([640, 640]): 135}
Pixel distribution by class id:
  0: 779257 pixels (1.41%)
  1: 1894107 pixels (3.43%)
  2: 1656969 pixels (3.00%)
  3: 2021292 pixels (3.66%)
  4: 2374293 pixels (4.29%)
  5: 8167986 pixels (14.77%)
  6: 12656469 pixels (22.89%)
  7: 129438 pixels (0.23%)
  8: 171539 pixels (0.31%)
  9: 25444650 pixels (46.02%)

=== TEST ===
Batches: 5
Images: 135
Image sizes (C,H,W): {(3, 640, 640): 135}
Mask sizes (H,W): {torch.Size([640, 640]): 135}
Pixel distribution by class id:
  0: 391920 pixels (0.71%)
  1: 830536 pixels (1.50%)
  2: 2491394 pixels (4.51%)
  3: 1449921 pixels (2.62%)
  4: 3541690 pixels (6.40%)
  5: 8118464 pixels (14.68%)
  6: 11138244 pixels (20.14%)
  7: 105962 pixels (0.19%)
  8: 162601 pixels (0.29%)
  9: 27065268 pixels (48.95%)